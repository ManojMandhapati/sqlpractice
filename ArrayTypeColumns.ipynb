{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eca2dd6-7cd5-4dee-9d4f-ada8f5392668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,ArrayType\n",
    "from pyspark.sql.functions import col,array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea00ca83-81b3-40e4-8d08-d7cdefda9912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('Manoj',[30,30000]), ('Megha',[27,40000]),('Manasa',[32,35000]),('Naveen',[34,38500]),('Vikky',[3,3000])]\n",
    "#schema = ['Name','Age_Salary']\n",
    "\n",
    "schema = StructType().add(field = 'Name',data_type = StringType())\\\n",
    "                    .add(field = 'Age_salary', data_type = ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e1b646-970f-472f-a0ce-26ddacc30187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n|  Name| Age_salary|\n+------+-----------+\n| Manoj|[30, 30000]|\n| Megha|[27, 40000]|\n|Manasa|[32, 35000]|\n|Naveen|[34, 38500]|\n| Vikky|  [3, 3000]|\n+------+-----------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Age_salary: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cb73fa-11eb-4639-9e65-74e3b98a6b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---+\n|  Name| Age_salary|Age|\n+------+-----------+---+\n| Manoj|[30, 30000]| 30|\n| Megha|[27, 40000]| 27|\n|Manasa|[32, 35000]| 32|\n|Naveen|[34, 38500]| 34|\n| Vikky|  [3, 3000]|  3|\n+------+-----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('Age',col('Age_salary')[0])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69288e55-54a6-4cf7-8c46-4b9e3a5a98ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,['Manoj','Mandhapati'],3000),\n",
    "        (2,['Megha','Mandhapati'],4000),\n",
    "        (3,['Manasa','Mandhapati'],3800),\n",
    "        (4,['Naveen','Jaggavarapu'],4000)]\n",
    "schema = ['id','name','salary']\n",
    "df = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f71c97-00c6-403c-805a-dcfcdb10496e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+-----------+\n| id|                name|salary| First_Last|\n+---+--------------------+------+-----------+\n|  1| [Manoj, Mandhapati]|  3000|      Manoj|\n|  1| [Manoj, Mandhapati]|  3000| Mandhapati|\n|  2| [Megha, Mandhapati]|  4000|      Megha|\n|  2| [Megha, Mandhapati]|  4000| Mandhapati|\n|  3|[Manasa, Mandhapati]|  3800|     Manasa|\n|  3|[Manasa, Mandhapati]|  3800| Mandhapati|\n|  4|[Naveen, Jaggavar...|  4000|     Naveen|\n|  4|[Naveen, Jaggavar...|  4000|Jaggavarapu|\n+---+--------------------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('First_Last',explode(col('name')))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1ff857-391a-4b20-8715-72360e7ddc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+------+\n| Id|  name|            skills|Salary|\n+---+------+------------------+------+\n|  1| Manoj|  Azure,Python,SQL|  3000|\n|  2| Megha| HPLC, Dissolution|  4000|\n|  3|Manasa|Manual, Automation|  3800|\n|  4|Naveen|       Jira, Agile|  4000|\n+---+------+------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,'Manoj','Azure,Python,SQL',3000),\n",
    "        (2,'Megha','HPLC, Dissolution',4000),\n",
    "        (3,'Manasa','Manual, Automation',3800),\n",
    "        (4,'Naveen','Jira, Agile',4000)]\n",
    "schema = ['Id','name','skills','Salary']\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d0528e-b6cb-4222-bf7c-656afd8781c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+------+--------------------+\n| Id|  name|            skills|Salary|         skillsArray|\n+---+------+------------------+------+--------------------+\n|  1| Manoj|  Azure,Python,SQL|  3000|[Azure, Python, SQL]|\n|  2| Megha| HPLC, Dissolution|  4000|[HPLC,  Dissolution]|\n|  3|Manasa|Manual, Automation|  3800|[Manual,  Automat...|\n|  4|Naveen|       Jira, Agile|  4000|      [Jira,  Agile]|\n+---+------+------------------+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "skills_df = df.withColumn('skillsArray',split(col('skills'),','))\n",
    "skills_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4edafb-4bc3-4dc3-bfa1-95b3a760de5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------------+------+\n| Id|  name|Primaryskill|Secondaryskill|Salary|\n+---+------+------------+--------------+------+\n|  1| Manoj|       Azure|        Python|  3000|\n|  2| Megha|        HPLC|   Dissolution|  4000|\n|  3|Manasa|      Manual|    Automation|  3800|\n|  4|Naveen|        Jira|         Agile|  4000|\n+---+------+------------+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,'Manoj','Azure','Python',3000),\n",
    "        (2,'Megha','HPLC', 'Dissolution',4000),\n",
    "        (3,'Manasa','Manual', 'Automation',3800),\n",
    "        (4,'Naveen','Jira', 'Agile',4000)]\n",
    "schema = ['Id','name','Primaryskill','Secondaryskill','Salary']\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429b9c37-8bf9-443d-af6e-f66b460f93d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------------+------+--------------------+\n| Id|  name|Primaryskill|Secondaryskill|Salary|              Skills|\n+---+------+------------+--------------+------+--------------------+\n|  1| Manoj|       Azure|        Python|  3000|     [Azure, Python]|\n|  2| Megha|        HPLC|   Dissolution|  4000| [HPLC, Dissolution]|\n|  3|Manasa|      Manual|    Automation|  3800|[Manual, Automation]|\n|  4|Naveen|        Jira|         Agile|  4000|       [Jira, Agile]|\n+---+------+------------+--------------+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "skills = df.withColumn('Skills',array(col('Primaryskill'),col('Secondaryskill')))\n",
    "skills.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db1d56c-ed0c-4d2b-b526-aae9f30f2460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+------+\n| Id|  name|              skills|Salary|\n+---+------+--------------------+------+\n|  1| Manoj|[Azure, Python, SQL]|  3000|\n|  2| Megha| [HPLC, Dissolution]|  4000|\n|  3|Manasa|[Manual, Automation]|  3800|\n|  4|Naveen|[Jira, Agile, Pyt...|  4000|\n+---+------+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,'Manoj',['Azure','Python','SQL'],3000),\n",
    "        (2,'Megha',['HPLC', 'Dissolution'],4000),\n",
    "        (3,'Manasa',['Manual', 'Automation'],3800),\n",
    "        (4,'Naveen',['Jira', 'Agile','Python'],4000)]\n",
    "schema = ['Id','name','skills','Salary']\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352bd24f-ae01-49c2-9bc2-95abefa7014d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ArrayTypeColumns",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
